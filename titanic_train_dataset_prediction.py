# -*- coding: utf-8 -*-
"""Titanic_Train_Dataset_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W27HfpNKbF6Eav8tBAGvJydEgrKBXj2e

## **1.) Importing the Required Libraries:**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
sns.set()
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""## **2.) Importing the dataset in pandas dataframe:**"""

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Titanic_Train_Dataset.csv')

data.head()

"""**About the dataset features**


* PassengerId - Unique Id of each passenger.
* Survived - '0' for not survived & '1' for survived
* Pclass - Passenger class: '1' for 1st class, '2' for 2nd class & '3' for 3rd class
* Name - Passenger name
* Sex - Passenger gender.
* Age - Passenger age
* SibSp - No. of siblings or spouses aborded.
* Parch - No. of parents or children aborded.
* Ticket - Passenger ticket number
* Fare - Passenger ticket fare
* Cabin - Passenger cabin number
* Embarked - Encoded name of city passenger embarked

After data importing and reading the data means the prediction will be in 0 or 1, Yes or No etc. which is very fitting in this Titanic scenario where the result we want to find out is alive or not.

My understanding on a logistics regression is that it is a classification model and it produces results in a binary format (discrete/categorical).
"""

data.shape

"""The titanic train data consists of 891 rows and 12 columns.

## **3.) Cleaning the data & Visualizing the features relation:**
"""

data.info()

data.describe()

data.isnull().sum()

"""**Visualiation**"""

f= plt.figure(figsize=(15,8))
sns.heatmap(data.isnull(),yticklabels = False , cbar = False , cmap = 'viridis')
plt.show()

"""Here, 
according to heatmap we have seen **Cabin** feature has more null value so that for better data we are dropping **'Cabin'** feature.
"""

data.drop("Cabin", axis=1, inplace=True)

data['Age'].fillna(data['Age'].mean(), inplace = True)

"""We have Replaced thie missing values in "Age" feature with mean value.

Here, we are finding the mode value of "Emmbarked" feature for fill the null value in "Emmbarked" feature.
"""

data['Embarked'].mode()

"""We are Replacing thie missing values in "Embarked" feature with mode value **"S"**:"""

data['Embarked'].fillna(data['Embarked'].mode()[0],inplace = True)

data.isnull().sum()

"""So now it is confirmed that the dataset is clean without any null value."""

f= plt.figure(figsize=(10,8))
sns.countplot('Survived', data = data)
plt.show()

"""Using seaborn library by the **countplot** we have ploted value counts of **"Survived"** feature for the better understanding of value.

But by this ploting we are not able to know exact value of "survived" feature, So that below we can use **value_counts**  method to calculate total value. 
"""

data['Survived'].value_counts()

"""As we show above '0' for not survived & '1' for survived. So by the checking value_counts function we have seen **549** passangers are not Survived & **342** passangers are Survived after the incident.
 
"""

f= plt.figure(figsize=(10,8))
sns.countplot(x="Survived", hue="Sex", data=data)
plt.show()

"""We can see females have a higher survival rate than males in this scenario."""

f = plt.figure(figsize=(10,8))
data["Age"].plot.hist()
plt.show()

"""Here, we can see average population of passengers on the titanic are young to middle age group."""

f = plt.figure(figsize = (10,8))
sns.boxplot(x="Pclass", y="Age", data=data)
plt.show()

"""By the **boxplot** we can see that older population of passengers are more likely to be in Passenger Class 1 & Class 2 than Class 3."""

correlation = data.corr()

plt.figure(figsize=(15,8))
sns_plot=sns.heatmap(data=correlation, annot=True, cmap="viridis")
plt.show()

data.head(2)

data.Sex.unique()

"""In the **"Sex"** feature we have 2 unique values only. So we can convert **'Sex'** feature values into '0' for female & '1' for male."""

data['Sex'] = data['Sex'].map({'female': 0,'male':1})

data.Embarked.unique()

data['Embarked'] = data['Embarked'].map({'S':0,'C':1,'Q':2})

"""Let's select usefull features for the better machine learning model prediction."""

x = data[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]
y = data['Survived']

x.head()

y.head()

"""## **4.) Spliting the data into training data & test data**"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)

print(x.shape,x_train.shape,x_test.shape)

"""##**5.) Training the Model**"""

model = LogisticRegression()

model.fit(x_train,y_train)

prediction = model.predict(x_test)
print(prediction)

accuracy_score(y_test,prediction)